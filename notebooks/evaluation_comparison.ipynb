{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9235afd7",
   "metadata": {},
   "source": [
    "# Evaluation Comparison Notebook\n",
    "\n",
    "This notebook walks through the complete 4-step evaluation workflow:\n",
    "1. **Initialize** the QA bot\n",
    "2. **Generate predictions** for test questions\n",
    "3. **Build the evaluation dataset** (questions + predictions + ground truth)\n",
    "4. **Score with an evaluation framework** (LangChain, DeepEval, RAGAS, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**LangChain + Ollama setup (optional)**\n",
    "- Install [Ollama](https://ollama.ai/download) and run `ollama pull llama3` (or any model you prefer).\n",
    "- In your `.env`, set `LANGCHAIN_USE_OLLAMA=true`, `OLLAMA_MODEL=llama3`, and optionally `OLLAMA_BASE_URL`.\n",
    "- Restart the notebook kernel after updating environment variables so LangChain picks up the new settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a97848",
   "metadata": {},
   "source": [
    "## Step 1: Initialize the QA Bot\n",
    "\n",
    "Set up the QA bot with the sample technical documentation. The bot will use TF-IDF retrieval to answer questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3b303e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ QA bot initialized with documents from C:\\Users\\Owner\\source\\repos\\LiteObject\\eval-framework-sandbox\\data\\documents\\sample_docs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import sys\n",
    "\n",
    "root = Path(\"..\").resolve()\n",
    "sys.path.insert(0, str(root))\n",
    "sys.path.insert(0, str(root / \"src\"))\n",
    "\n",
    "from src.qa_bot import QABot\n",
    "from evaluations.base_evaluator import EvaluationInput\n",
    "\n",
    "# Step 1: Initialize the bot with sample technical documentation\n",
    "bot = QABot(documents_path=root / \"data\" / \"documents\" / \"sample_docs\")\n",
    "print(\n",
    "    f\"✓ QA bot initialized with documents from {root / 'data' / 'documents' / 'sample_docs'}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d082acb",
   "metadata": {},
   "source": [
    "## Steps 2 & 3: Generate Predictions & Build Evaluation Dataset\n",
    "\n",
    "Ask the bot each test question to generate **predictions**, then pair them with **ground truth** answers to create the evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df337e58",
   "metadata": {},
   "source": [
    "### What is a prediction?\n",
    "\n",
    "In this notebook, a **prediction** is simply the answer text produced by the QA bot for each test question. Evaluation frameworks compare that prediction against the corresponding **ground truth** answer to compute metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3cc5e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated 3 predictions\n",
      "✓ Prepared 3 evaluation samples with predictions & ground truth\n"
     ]
    }
   ],
   "source": [
    "from evaluations.utils import load_dataset_from_files\n",
    "\n",
    "questions_path = root / \"data\" / \"test_questions.json\"\n",
    "ground_truth_path = root / \"data\" / \"ground_truth.json\"\n",
    "\n",
    "# Step 2: Generate predictions from the bot for each test question\n",
    "questions_data = json.loads(questions_path.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "predictions: dict[str, str] = {}\n",
    "for item in questions_data:\n",
    "    response = bot.answer(item[\"question\"])\n",
    "    predictions[item[\"id\"]] = response.response\n",
    "\n",
    "print(f\"✓ Generated {len(predictions)} predictions\")\n",
    "\n",
    "# Step 3: Build the evaluation dataset (predictions + ground truth)\n",
    "eval_dataset = list(\n",
    "    load_dataset_from_files(\n",
    "        questions_path=questions_path,\n",
    "        ground_truth_path=ground_truth_path,\n",
    "        predictions=predictions,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    f\"✓ Prepared {len(eval_dataset)} evaluation samples with predictions & ground truth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c75894",
   "metadata": {},
   "source": [
    "## Step 4: Score with Evaluation Frameworks\n",
    "\n",
    "Run one or more evaluation frameworks to score how well your QA bot performed. Start with LangChain, then try others (DeepEval, RAGAS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fdc24f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== LangChain Evaluation ===\n",
      "✓ Score: 0.3333333333333333\n",
      "✓ Score: 0.3333333333333333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'framework': 'langchain',\n",
       " 'score': 0.3333333333333333,\n",
       " 'details': {'raw': [{'reasoning': 'CORRECT', 'value': 'CORRECT', 'score': 1},\n",
       "   {'reasoning': 'INCORRECT', 'value': 'INCORRECT', 'score': 0},\n",
       "   {'reasoning': 'INCORRECT', 'value': 'INCORRECT', 'score': 0}],\n",
       "  'provider': 'ollama'}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from evaluations.langchain_eval_runner import LangChainEvalRunner\n",
    "\n",
    "# Step 4a: LangChain Evaluation\n",
    "print(\"\\n=== LangChain Evaluation ===\")\n",
    "langchain_runner = LangChainEvalRunner(output_dir=root / \"results\")\n",
    "try:\n",
    "    langchain_result = langchain_runner.evaluate(eval_dataset)\n",
    "    langchain_summary = {\n",
    "        \"framework\": langchain_result.framework,\n",
    "        \"score\": langchain_result.score,\n",
    "        \"details\": langchain_result.details,\n",
    "    }\n",
    "    print(f\"✓ Score: {langchain_result.score}\")\n",
    "except Exception as exc:\n",
    "    langchain_summary = {\n",
    "        \"framework\": \"langchain\",\n",
    "        \"error\": str(exc),\n",
    "    }\n",
    "    print(f\"✗ Error: {exc}\")\n",
    "\n",
    "langchain_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0f193b",
   "metadata": {},
   "source": [
    "[\"## Step 4b: Compare Framework Scores\",\"\",\"Collect results from all evaluated frameworks and display a summary comparison.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb927c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Framework Comparison ===\n",
      "LangChain: 0.333\n"
     ]
    }
   ],
   "source": [
    "# Collect results from all frameworks (extend this dict as you run more evaluations)\n",
    "framework_results = {\n",
    "    \"LangChain\": langchain_summary.get(\"score\"),\n",
    "    # \"DeepEval\": deepeval_summary.get(\"score\"),\n",
    "    # \"RAGAS\": ragas_summary.get(\"score\"),\n",
    "}\n",
    "\n",
    "print(\"\\n=== Framework Comparison ===\")\n",
    "for framework, score in framework_results.items():\n",
    "    if score is not None:\n",
    "        print(f\"{framework}: {score:.3f}\")\n",
    "    else:\n",
    "        print(f\"{framework}: Not evaluated or error occurred\")\n",
    "\n",
    "# Optional: Create a simple bar chart if you have multiple results\n",
    "if len([s for s in framework_results.values() if s is not None]) > 1:\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        names = [k for k, v in framework_results.items() if v is not None]\n",
    "        scores = [v for v in framework_results.values() if v is not None]\n",
    "        plt.bar(names, scores)\n",
    "        plt.ylabel(\"Score (0-1)\")\n",
    "        plt.title(\"Evaluation Framework Comparison\")\n",
    "        plt.show()\n",
    "    except ImportError:\n",
    "        print(\"Install matplotlib for visualization: pip install matplotlib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11caa919",
   "metadata": {},
   "source": [
    "## Optional: Try Other Frameworks\n",
    "\n",
    "Uncomment and run any of the cells below to compare scores across frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a636dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluations.deepeval_runner import DeepEvalRunner\n",
    "\n",
    "# print(\"\\n=== DeepEval Evaluation ===\")\n",
    "# deepeval_runner = DeepEvalRunner(output_dir=root / \"results\")\n",
    "# try:\n",
    "#     deepeval_result = deepeval_runner.evaluate(eval_dataset)\n",
    "#     print(f\"✓ Score: {deepeval_result.score}\")\n",
    "#     deepeval_result.details\n",
    "# except Exception as exc:\n",
    "#     print(f\"✗ Error: {exc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b777fe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from evaluations.ragas_runner import RagasRunner\n",
    "\n",
    "# print(\"\\n=== RAGAS Evaluation ===\")\n",
    "# ragas_runner = RagasRunner(output_dir=root / \"results\")\n",
    "# try:\n",
    "#     ragas_result = ragas_runner.evaluate(eval_dataset)\n",
    "#     print(f\"✓ Score: {ragas_result.score}\")\n",
    "#     ragas_result.details\n",
    "# except Exception as exc:\n",
    "#     print(f\"✗ Error: {exc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
